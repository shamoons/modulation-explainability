# W&B Sweep Configuration for Architecture Comparison
# Task-specific feature extraction across different model architectures
# Bayesian optimization with early stopping for efficient exploration

program: src/train_constellation.py
method: bayes  # Bayesian optimization for intelligent hyperparameter search
project: modulation-explainability  # Explicitly set the correct project

# Sweep parameters - Bayesian will intelligently sample these ranges
parameters:
  # Model architectures to compare
  model_type:
    values: [resnet18, resnet34, vit_b_32, vit_b_16, vit_h_14, swin_tiny]
  
  # Batch sizes optimized for memory efficiency - moderate sizes for stable training
  batch_size:
    values: [64, 128, 256, 512]  # Memory-efficient batch sizes
  
  # Dropout regularization - discrete steps
  dropout:
    values: [0.1, 0.25, 0.5]
  
  # Learning rate - discrete order of magnitude steps
  base_lr:
    values: [1e-5, 1e-4, 1e-3]

  # Weight decay - discrete order of magnitude steps  
  weight_decay:
    values: [1e-6, 1e-5, 1e-4, 1e-3]

  # Fixed parameters
  epochs:
    value: 50
  
  patience:
    value: 3

# Sweep metadata
name: "bayes_memory_efficient_comparison"
description: "Memory-efficient Bayesian optimization with moderate batch sizes (128-512) for stable training across all architectures"

# Aggressive early termination - stop poor performers quickly
early_terminate:
  type: hyperband
  min_iter: 3      # Stop bad runs after just 3 epochs
  max_iter: 50     # Max epochs if run is promising
  s: 3             # More aggressive elimination
  eta: 2           # Faster halving

# Metric to optimize with Bayesian acquisition
metric:
  name: val_combined_accuracy
  goal: maximize