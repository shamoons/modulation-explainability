# W&B Sweep Configuration - Architecture Comparison with Optimized Parameters
# Testing both ResNet34 and Swin with architecture-specific hyperparameters
# Includes LR warmup options and longer training

program: src/train_constellation.py
method: bayes  # Continue Bayesian optimization for final tuning
project: modulation-classification

# Parameters with some architecture-specific considerations
parameters:
  # Model architectures to compare
  model_type:
    values: [resnet34, resnet50, swin_tiny, swin_small, vit_b_16]  # Full architecture comparison
  
  # SNR layer configurations - all options
  snr_layer_config:
    values: [bottleneck_64, bottleneck_128, standard, dual_layer]  # All 4 configurations
  
  # Batch size - smaller sizes to accommodate all models
  batch_size:
    values: [64, 128, 256]  # Smaller sizes for memory constraints, especially for transformers
  
  # Dropout - fixed to universal best
  dropout:
    value: 0.5
  
  # Learning rates - wider range to accommodate both architectures
  base_lr:
    values: [1e-6, 1e-5, 5e-5]  # Lower options for Swin
  
  max_lr:
    values: [1e-4, 5e-4, 1e-3]  # 1e-4 for Swin, 5e-4+ for ResNet
  
  # LR Warmup parameters
  warmup_epochs:
    values: [0, 5, 10]  # 0 = no warmup, 5-10 for transformers
  
  warmup_start_factor:
    values: [0.1, 0.3]  # Start LR as fraction of base_lr
  
  # Weight decay - focus on top values
  weight_decay:
    values: [1e-5, 1e-4]  # Remove 1e-3 (caused crashes)
  
  # Pretrained weights
  use_pretrained:
    values: [true, false]  # Swin benefits from pretraining
  
  # Extended training for better convergence
  epochs:
    value: 100  # Double from 50 to allow full convergence
  
  patience:
    value: 10  # Double patience for longer runs
  
  # Fixed SNR range
  snr_list:
    value: "0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30"
  
  
  

# Sweep metadata
name: "architecture_comparison_quick_start"
description: "Quick architecture comparison with ResNet34/50, Swin Tiny/Small, and ViT. Using smaller batch sizes (64-256) and aggressive early stopping (5-20 epochs) to quickly identify promising configurations. All SNR layer configs included."

# More aggressive early termination for initial exploration
early_terminate:
  type: hyperband
  min_iter: 5      # Quick initial assessment
  max_iter: 20     # Stop early to test more configurations
  s: 2             # More aggressive pruning
  eta: 3           # Faster halving

# Track best validation accuracy
metric:
  name: val_combined_accuracy
  goal: maximize