# W&B Sweep Configuration - Optimized ResNet34 Parameters
# Based on Bayesian sweep analysis showing ResNet34 + bottleneck_128 dominance
# Focus on best performing hyperparameters with increased batch sizes

program: src/train_constellation.py
method: bayes  # Continue using Bayesian optimization for fine-tuning
project: modulation-classification

# Optimized parameters based on sweep analysis
parameters:
  # Model architectures - ResNet34 dominated but keeping ResNet18 for comparison
  model_type:
    values: [resnet34, resnet18]  # ResNet34: 46.9% best, ResNet18: 34.1%
  
  # SNR layer configurations - bottleneck_128 in 4/5 top runs  
  snr_layer_config:
    values: [bottleneck_128, bottleneck_64]  # Top 2 configurations
  
  # Batch sizes - increased for better GPU utilization with ResNet
  batch_size:
    values: [256, 512, 1024]  # 256 was best, testing 2x and 4x larger
  
  # Dropout - 0.5 used in ALL top 5 runs
  dropout:
    value: 0.5  # Fixed to optimal value
  
  # Base learning rate - narrowed range based on best runs
  base_lr:
    values: [1e-6, 1e-5]  # Best run used 1e-6, second best used 1e-5
  
  # Max learning rate - top 2 values from analysis
  max_lr:
    values: [1e-4, 5e-4]  # 5e-4 gave 46.9%, 1e-4 most stable (42.2% avg)

  # Weight decay - top 3 values
  weight_decay:
    values: [1e-5, 1e-4, 1e-3]  # 1e-5 best (46.9%), others common
  
  # Pretrained weights - both competitive
  use_pretrained:
    values: [true, false]  # true: 46.9%, false: 44.4%

  # Fixed parameters
  epochs:
    value: 50
  
  patience:
    value: 5
  
  # SNR range for experiments
  snr_list:
    value: "0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30"

# Sweep metadata
name: "optimized_resnet_large_batch"
description: "Optimized ResNet34/18 sweep with best hyperparameters from analysis. Testing larger batch sizes (256-1024) for improved GPU utilization. Focus on bottleneck_128/64 SNR layers with proven learning rates. Extended min_iter=8 for better optimization."

# More patient early termination - let W&B optimizer run longer
early_terminate:
  type: hyperband
  min_iter: 8      # Give runs even more time before stopping
  max_iter: 50     # Max epochs if run is promising
  s: 2             # Less aggressive elimination
  eta: 3           # Slower halving for better configs

# Use validation accuracy since we may not reach final test
metric:
  name: val_combined_accuracy
  goal: maximize