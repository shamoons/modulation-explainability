# W&B Sweep Configuration for Architecture Comparison
# Task-specific feature extraction across different model architectures
# Bayesian optimization with early stopping for efficient exploration

program: src/train_constellation.py
method: bayes  # Bayesian optimization for intelligent hyperparameter search
project: modulation-explainability  # Explicitly set the correct project

# Sweep parameters - Bayesian will intelligently sample these ranges
parameters:
  # Model architectures to compare
  model_type:
    values: [resnet18, resnet34, vit_b_32, swin_tiny]
  
  # Batch sizes optimized for memory constraints
  batch_size:
    values: [32, 64, 128]  # Bayesian will pick optimal for each architecture
  
  # Dropout regularization - discrete steps
  dropout:
    values: [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4]
  
  # Learning rate - discrete order of magnitude steps
  base_lr:
    values: [3e-5, 5e-5, 8e-5, 1e-4, 1.5e-4, 2e-4]

  # Weight decay - discrete order of magnitude steps  
  weight_decay:
    values: [1e-6, 5e-6, 1e-5, 5e-5, 1e-4]

  # Fixed parameters
  epochs:
    value: 50
  
  patience:
    value: 3

# Sweep metadata
name: "bayes_architecture_comparison"
description: "Bayesian optimization across ResNet/ViT/Swin architectures with task-specific features and early stopping"

# Aggressive early termination - stop poor performers quickly
early_terminate:
  type: hyperband
  min_iter: 3      # Stop bad runs after just 3 epochs
  max_iter: 50     # Max epochs if run is promising
  s: 3             # More aggressive elimination
  eta: 2           # Faster halving

# Metric to optimize with Bayesian acquisition
metric:
  name: val_combined_accuracy
  goal: maximize