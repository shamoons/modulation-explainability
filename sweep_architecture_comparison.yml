# W&B Sweep Configuration for Architecture Comparison
# Testing different architectures and SNR layer configurations
# Using standard cross-entropy loss for both tasks

program: src/train_constellation.py
method: bayes  # Bayesian optimization for intelligent hyperparameter search
project: modulation-classification  # Updated project name

# Sweep parameters - Bayesian will intelligently sample these ranges
parameters:
  # Model architectures to compare
  model_type:
    values: [resnet18, resnet34, swin_tiny, swin_small]
  
  # SNR layer configurations
  snr_layer_config:
    values: [standard, bottleneck_64, bottleneck_128, dual_layer]
  
  # Batch sizes - discrete values for stability
  batch_size:
    values: [64, 128, 256]
  
  # Dropout regularization - discrete values
  dropout:
    values: [0.1, 0.25, 0.5]
  
  # Learning rate - discrete values based on successful runs
  base_lr:
    values: [1e-6, 1e-5, 1e-4]
  
  # Max learning rate for CyclicLR - discrete values
  max_lr:
    values: [1e-4, 5e-4, 1e-3]

  # Weight decay - discrete values
  weight_decay:
    values: [1e-5, 1e-4, 1e-3]
  
  # Whether to use pretrained weights (Swin models only)
  use_pretrained:
    values: [true, false]

  # Fixed parameters
  epochs:
    value: 50
  
  patience:
    value: 5
  
  # SNR range for experiments
  snr_list:
    value: "0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30"

# Sweep metadata
name: "bayes_memory_efficient_comparison"
description: "Memory-efficient Bayesian optimization with moderate batch sizes (128-512) for stable training across all architectures"

# Aggressive early termination - stop poor performers quickly
early_terminate:
  type: hyperband
  min_iter: 3      # Stop bad runs after just 3 epochs
  max_iter: 50     # Max epochs if run is promising
  s: 3             # More aggressive elimination
  eta: 2           # Faster halving

# Metric to optimize with Bayesian acquisition
metric:
  name: val_combined_accuracy
  goal: maximize