\documentclass{ELSP}
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
	\usepackage[]{microtype}
	\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\IfFileExists{parskip.sty}{%
	\usepackage{parskip}
}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
	pdfcreator={ELSP}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
	\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{times}
\usepackage{mathptmx}
\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{geometry}
\geometry{a4paper,top=2.5cm,bottom=1.9cm,left=1.75cm,right=1.75cm,headsep=12px}
\usepackage{float}
\usepackage{enumerate}
\usepackage{stfloats}
\usepackage{ragged2e}
\usepackage{titlesec}
%\titleformat*{\section}{\fontsize{12pt}{12pt}\selectfont\bfseries}
%\titleformat*{\subsection}{\fontsize{12pt}{12pt}\selectfont\emph}
%\titleformat*{\subsubsection}{\fontsize{12pt}{12pt}\selectfont}

\titleformat{\section}[hang]
  {\fontsize{12pt}{12pt}\selectfont\bfseries\color[RGB]{0,131,255}} 
  {\thesection}{0.5em}{}


\titleformat{\subsection}[hang]
  {\fontsize{12pt}{12pt}\selectfont\emph} 
  {\thesubsection}{0.5em}{}


\titleformat{\subsubsection}[hang]
  {\fontsize{12pt}{12pt}\selectfont} 
  {\thesubsubsection}{0.5em}{}

\setlength{\parindent}{2em}
\setlength{\baselineskip}{17pt}
\setlength{\parskip}{12pt}


% \setlength{\bibitemsep}{\baselineskip}
% \setlength{\parskip}{12pt}

% \usepackage{fontspec}
% \setmainfont{Times New Roman}
%ELSP fancy
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[R]{\sffamily \footnotesize \textbf{\textcolor[RGB]{0,131,255}{Paper type}}}
\fancyhead[L]{\sffamily \footnotesize \textbf{\textcolor[RGB]{0,131,255}{Journal}}}
\fancyfoot[R]{\thepage}
\renewcommand{\headrulewidth}{1.4pt}
\fancypagestyle{firstpage}
{
	\fancyhf{}
	\setlength{\headsep}{6px}
	\fancyhead[R]{\sffamily \footnotesize \textbf{\textcolor[RGB]{0,131,255}{Journal}}}
	\fancyhead[L]{\sffamily \footnotesize \textbf{\textcolor[RGB]{0,131,255}{ELSP}}}
	%\fancyfoot[R]{\thepage}
	\fancyfoot[L]{\sffamily \footnotesize Author {\em et al}. Journal Year (Issue): XXXX}
	\renewcommand{\headrulewidth}{1.4pt}
}

\captionsetup{labelfont={bf},labelformat={default},labelsep=period,margin=4em,format=plain}

\begin{document}



%\hfill \textbf{\large Type of the paper}
\thispagestyle{firstpage}

\let\thefootnote\relax
\footnotetext{
\newline
\newline
	\begin{minipage}[h]{0.15\linewidth}
	\includegraphics{fig/cc.png}
	\end{minipage}
	\hfill
	\begin{minipage}[h]{0.8\linewidth}
		\footnotesize{Copyright©Year by the authors. Published by ELSP. 
			This work is licensed under a Creative Commons Attribution 4.0 
			International License, which permits unrestricted use, distribution, 
			and reproduction in any medium provided the original work is properly cited}
	\end{minipage}

}

\setstretch{1.24}
\begin{flushleft}
% \papertype{paper type}
{\sffamily \small \noindent {Paper Type $\mid$ Received Day Mon Year; Accepted Day Mon Year; Published Day Mon Year}}\\
%Example
%\timeline{15 May 2022}{17 July 2022}{20 September 2022}
{\sffamily\small{https://doi.org/10.55092/xxxx}}

\papertitle{Joint Modulation-SNR Classification with Perturbation-Based Explainability for Automatic Modulation Classification}

\authorname{Shamoon} {Siddiqui} {1}
\textbf{and} 
\authornameCorres{Ravi} {Ramachandran}{1,}{*}

\formatintroduction{1}{Department of Electrical and Computer Engineering, Rowan University, Glassboro, NJ, USA}

\authoremail{Correspondence author} {E-mail: siddiq76@rowan.edu}
\end{flushleft}
\noindent\textbf{\textcolor[RGB]{0,131,255}{Highlights:}}\\
\newline
Highlights are three to five bullet points that help increase the discoverability of your article via search engines. These bullet points should capture the novel results of your research as well as new methods that were used during the study (if any). Think of them as the "elevator pitch" of your article. Please include terms that you know your readers will be looking for online. Don't try to capture all ideas, concepts or conclusions as highlights are meant to be short: 85 characters or fewer, including spaces.
\begin{itemize}
    \item First joint 272-class modulation-SNR classification without oracle SNR
    \item 51.26\% test accuracy using ResNet50 with curriculum learning
    \item SNR-preserving preprocessing achieves 5.7x improvement over baselines
    \item Perturbation Impact Score (PIS) for constellation diagram explainability
    \item Comprehensive evaluation against cascade and oracle-based approaches
\end{itemize}

\noindent\textbf{\textbf{\textcolor[RGB]{0,131,255}{Abstract:}}} Automatic modulation classification (AMC) is critical for cognitive radio and electronic warfare applications, yet existing approaches rely on cascade methods with oracle signal-to-noise ratio (SNR) estimates, limiting practical deployment. This paper presents the first comprehensive study of joint modulation-SNR classification, simultaneously predicting both modulation type and SNR level without prior knowledge. We formulate this as a 272-class problem combining 17 digital modulations with 16 SNR levels (0-30 dB), representing a 10× increase in complexity over existing joint approaches. Our method employs a ResNet50 architecture with a specialized bottleneck SNR layer, trained using curriculum learning with a sliding window approach. A novel SNR-preserving preprocessing technique using power normalization maintains critical signal strength information, achieving 5.7× improvement over traditional per-image normalization. The model achieves 51.26\% test accuracy (76.39\% modulation, 68.71\% SNR) on the RadioML2018 dataset, establishing a new benchmark for joint AMC. To provide interpretability, we introduce the Perturbation Impact Score (PIS) for constellation diagram explainability, identifying critical regions that influence classification decisions. Comprehensive evaluation against cascade and oracle-based approaches demonstrates the effectiveness of our joint prediction framework, addressing practical limitations of existing methods while maintaining competitive performance. [TBD: Statistical significance testing results and additional ablation study quantification]

\noindent\textbf{\textcolor[RGB]{0,131,255}{Keywords:}} automatic modulation classification, signal-to-noise ratio, deep learning, ResNet, curriculum learning, constellation diagrams, explainable AI, perturbation analysis, joint classification, cognitive radio 


\section{Introduction}

Automatic modulation classification (AMC) is a fundamental capability in modern communications systems, serving as the cornerstone for cognitive radio networks, electronic warfare, and spectrum monitoring applications~\cite{oshea2017introduction,dobre2007survey}. The ability to automatically identify modulation schemes from received signals enables adaptive communication systems to optimize transmission parameters, detect unauthorized transmissions, and implement intelligent jamming countermeasures~\cite{west2017deep,liu2020survey}. However, current AMC approaches face significant limitations when deployed in practical scenarios where both modulation type and signal quality must be determined simultaneously.

Traditional AMC methods follow a cascade approach, first estimating the signal-to-noise ratio (SNR) using dedicated algorithms, then applying SNR-specific classifiers for modulation identification~\cite{zhang2023multimodal,chen2024preclassification}. While these two-stage methods achieve high accuracy under controlled conditions, they suffer from fundamental limitations in real-world deployment. The cascade approach requires oracle SNR knowledge, which is often unavailable or unreliable in dynamic environments~\cite{wang2024wctformer}. Moreover, errors in SNR estimation propagate through the classification pipeline, leading to degraded performance when the estimated SNR deviates from the true value~\cite{pauluzzi2000comparison}. Recent work by Chen et al.~\cite{chen2024preclassification} achieved 99\% accuracy at 10 dB SNR, but only with perfect SNR pre-classification, highlighting the practical limitations of cascade methods.

Joint modulation-SNR classification represents a paradigm shift from cascade approaches, simultaneously predicting both modulation type and SNR level without requiring oracle knowledge. However, this joint formulation presents substantial challenges due to the exponential increase in classification complexity. While traditional AMC addresses 17 modulation classes and SNR estimation handles continuous values, joint classification must distinguish between $17 \times 16 = 272$ discrete classes, representing a 10× increase in complexity compared to existing joint approaches~\cite{liu2022jointsnr}. This complexity explosion, combined with the fundamental tension between modulation-specific geometric patterns and SNR-dependent noise characteristics, has limited previous joint approaches to modest performance levels.

Constellation diagram analysis has emerged as a powerful approach for AMC, leveraging the geometric properties of signal representations in the complex plane~\cite{peng2023constellation,gao2023robust}. Unlike time-domain methods that process raw I/Q samples, constellation diagrams provide intuitive visualizations of modulation schemes while maintaining essential signal characteristics. However, conventional constellation preprocessing techniques destroy critical SNR information through per-image normalization, limiting their effectiveness for joint classification tasks~\cite{meng2018automatic,tu2019deep}. The development of SNR-preserving preprocessing methods remains an open challenge, requiring careful balance between noise suppression and signal strength preservation.

Explainability in AMC systems has gained increasing attention as deep learning methods become prevalent in critical applications~\cite{arrieta2020explainable,samek2019explainable}. Traditional black-box classifiers provide limited insight into decision-making processes, hindering deployment in security-sensitive scenarios where understanding model behavior is crucial. While explainable AI (XAI) methods like SHAP~\cite{lundberg2017shap} and LIME~\cite{ribeiro2016lime} offer general-purpose explanations, they lack domain-specific insights for constellation diagrams. Perturbation-based explanation methods~\cite{fong2017interpretable,petsiuk2018rise} show promise for image classification tasks, but their application to constellation diagrams in joint AMC remains unexplored.

This paper addresses these limitations by presenting the first comprehensive study of joint modulation-SNR classification with perturbation-based explainability. Our key contributions include: (1) A joint classification framework that simultaneously predicts modulation type and SNR level without oracle knowledge, formulated as a 272-class problem; (2) A novel SNR-preserving preprocessing technique using power normalization that maintains critical signal strength information while enabling effective constellation diagram analysis; (3) A ResNet50-based architecture with a specialized bottleneck SNR layer, trained using curriculum learning with a sliding window approach to manage the complexity of joint classification; (4) The introduction of the Perturbation Impact Score (PIS) for constellation diagram explainability, providing interpretable insights into model decision-making processes; and (5) Comprehensive evaluation demonstrating 51.26\% test accuracy on the RadioML2018 dataset, establishing a new benchmark for joint AMC while addressing practical limitations of existing cascade approaches.

The remainder of this paper is organized as follows. Section~\ref{sec:related_work} reviews related work in cascade AMC approaches, constellation-based methods, and explainable AI for signal processing. Section~\ref{sec:methodology} presents our joint classification framework, SNR-preserving preprocessing, and PIS explainability metric. Section~\ref{sec:results} provides comprehensive experimental evaluation and ablation studies. Section~\ref{sec:discussion} discusses implications and limitations, and Section~\ref{sec:conclusion} concludes with future work directions.

[TBD: Add section labels and verify all citations are in ref.bib]


\section{Related Work}
\label{sec:related_work}

This section reviews existing approaches to automatic modulation classification, focusing on cascade versus joint methods, constellation-based techniques, and explainable AI applications in signal processing.

\subsection{Cascade vs Joint Approaches}

The majority of existing AMC systems follow a cascade architecture, where SNR estimation and modulation classification are performed sequentially. Zhang et al.~\cite{zhang2023multimodal} developed a multi-modal approach using both time-domain signals and constellation diagrams, but with a critical limitation: below -4 dB SNR, the system relies exclusively on time-domain processing, effectively abandoning constellation analysis in low-SNR scenarios. This threshold-based approach highlights the fundamental challenge of cascade methods in handling the full SNR spectrum.

Chen et al.~\cite{chen2024preclassification} achieved impressive 99\% accuracy at 10 dB SNR through pre-classification and SNR-specific compensation. However, their approach assumes perfect SNR knowledge, representing an oracle-based method that cannot be deployed in practical scenarios where SNR estimates are uncertain. Similarly, the WCTFormer approach by Wang et al.~\cite{wang2024wctformer} achieved 97.8\% accuracy but relied on oracle SNR labels, limiting its real-world applicability.

These cascade approaches suffer from error propagation, where inaccuracies in SNR estimation compound through the classification pipeline. Pauluzzi and Beaulieu~\cite{pauluzzi2000comparison} demonstrated that SNR estimation errors in AWGN channels can significantly impact downstream classification performance, particularly when the estimated SNR deviates from the true value by more than 2-3 dB.

Joint modulation-SNR classification represents a paradigm shift from cascade methods, but existing approaches have achieved limited success. Liu and Wong~\cite{liu2022jointsnr} attempted joint classification on a 220-class problem, achieving only 28.4\% accuracy. Chen et al.~\cite{chen2021endtoend} explored end-to-end joint prediction but faced similar performance limitations. These results highlight the exponential complexity increase when moving from separate estimation tasks to joint classification, motivating our comprehensive study of the 272-class problem.

\subsection{Constellation-Based AMC}

Constellation diagrams provide intuitive visualizations of modulation schemes by representing signal samples in the complex plane. Traditional approaches relied on geometric feature extraction, using measures such as constellation shape, clustering patterns, and statistical moments~\cite{swami2000hierarchical,azzouz1996automatic}. However, these hand-crafted features often fail to capture the subtle differences between modulation schemes, particularly in noisy environments.

Deep learning has revolutionized constellation-based AMC, with convolutional neural networks (CNNs) automatically learning discriminative features from constellation images. Meng et al.~\cite{meng2018automatic} demonstrated that CNN-based approaches significantly outperform traditional feature extraction methods. Zhang et al.~\cite{zhang2022resnet} explored ResNet variants with multi-size convolution kernels, achieving improved performance through hierarchical feature extraction.

Recent work has focused on architecture optimization for constellation analysis. Park et al.~\cite{park2023lenet} introduced LENet-L with large-kernel enhancements specifically for constellation-based recognition. Liu et al.~\cite{liu2023robust} developed SNR-specific routing mechanisms within CNN architectures, attempting to address the joint classification challenge through architectural modifications.

However, a critical limitation in existing constellation-based methods is the destruction of SNR information through preprocessing. Traditional approaches apply per-image normalization to enhance visual contrast, inadvertently removing the signal strength information necessary for SNR estimation~\cite{tu2019deep,han2020modulation}. This preprocessing bottleneck has prevented effective joint modulation-SNR classification using constellation diagrams.

SNR range considerations have also emerged as a practical constraint. Peng et al.~\cite{peng2023constellation} focused on practical SNR ranges of 0-30 dB, arguing that extreme SNR conditions are less relevant for real-world applications. García-López et al.~\cite{garcia2024ultralight} achieved 96.3\% accuracy but limited evaluation to 0-20 dB, avoiding the challenging high-SNR regime where constellation patterns become less discriminative.

\subsection{Explainable AI for Signal Processing}

The increasing deployment of deep learning in critical applications has driven demand for explainable AI (XAI) methods. Traditional black-box classifiers provide limited insight into decision-making processes, hindering deployment in security-sensitive scenarios where understanding model behavior is crucial~\cite{arrieta2020explainable,adadi2018peeking}.

General-purpose XAI methods like SHAP~\cite{lundberg2017shap} and LIME~\cite{ribeiro2016lime} provide model-agnostic explanations but lack domain-specific insights for signal processing tasks. These methods treat input features as independent variables, failing to capture the spatial relationships inherent in constellation diagrams.

Perturbation-based explanation methods have shown promise for image classification tasks. Fong and Vedaldi~\cite{fong2017interpretable} introduced meaningful perturbation techniques that identify critical image regions through systematic occlusion. Petsiuk et al.~\cite{petsiuk2018rise} developed randomized input sampling for explanation, providing probabilistic importance scores for image regions.

However, the application of XAI methods to signal processing remains limited. Ismail et al.~\cite{ismail2020benchmarking} benchmarked XAI methods for time series predictions, finding that perturbation-based approaches often outperform gradient-based methods in terms of explanation quality. Theissler et al.~\cite{theissler2022explainable} provided a comprehensive review of XAI for time series classification, but constellation diagram analysis was not addressed.

The gap in XAI research for joint modulation-SNR classification is particularly pronounced. Existing explanation methods focus on single-task problems, lacking the multi-task understanding necessary for joint classification scenarios. Moreover, the unique properties of constellation diagrams—where both geometric patterns and noise characteristics influence decisions—require specialized explanation techniques that consider signal-specific interpretations.

[TBD: Verify all citations are properly formatted and available in ref.bib]

\section{Methodology}
\label{sec:methodology}

This section presents our joint modulation-SNR classification framework, including problem formulation, SNR-preserving preprocessing, architecture design, training strategy, and the Perturbation Impact Score (PIS) explainability metric.

\subsection{Problem Formulation}

We formulate joint modulation-SNR classification as a multi-task learning problem, where a single model simultaneously predicts modulation type $m$ and SNR level $s$ from constellation diagram input $X$. Given $M$ modulation classes and $S$ SNR levels, the joint prediction space contains $M \times S$ possible combinations.

Let $f_\theta: \mathbb{R}^{H \times W \times C} \rightarrow \mathbb{R}^M \times \mathbb{R}^S$ represent our neural network with parameters $\theta$, where $H$, $W$, and $C$ are the height, width, and channels of input constellation diagrams. The network produces two outputs: modulation predictions $\hat{y}_m = f_\theta^{(m)}(X)$ and SNR predictions $\hat{y}_s = f_\theta^{(s)}(X)$.

The multi-task learning objective combines modulation and SNR losses using uncertainty weighting~\cite{kendall2018multitask}:
\begin{equation}
\mathcal{L} = \frac{1}{2\sigma_m^2}\mathcal{L}_m + \frac{1}{2\sigma_s^2}\mathcal{L}_s + \log(\sigma_m\sigma_s)
\end{equation}
where $\mathcal{L}_m$ and $\mathcal{L}_s$ are the modulation and SNR losses, respectively, and $\sigma_m$, $\sigma_s$ are learnable uncertainty parameters that automatically balance task contributions during training.

For our 272-class problem, we use $M = 17$ digital modulations (BPSK, QPSK, 8PSK, 16PSK, 32PSK, OQPSK, 4ASK, 8ASK, 16QAM, 32QAM, 64QAM, 128QAM, 256QAM, 16APSK, 32APSK, 64APSK, 128APSK) and $S = 16$ SNR levels (0, 2, 4, ..., 30 dB), representing the practical operating range for constellation-based AMC~\cite{peng2023constellation}.

\subsection{SNR-Preserving Preprocessing}

A critical challenge in joint modulation-SNR classification is preserving signal strength information during constellation diagram generation. Traditional preprocessing approaches apply per-image normalization to enhance visual contrast, inadvertently destroying the SNR information necessary for signal quality estimation.

Our SNR-preserving preprocessing technique uses power normalization to maintain relative signal strength while enabling effective visualization:

\begin{equation}
\begin{aligned}
P &= \frac{1}{N} \sum_{i=1}^{N} (I_i^2 + Q_i^2) \\
\alpha &= \sqrt{P} \\
I_{norm} &= \frac{I}{\alpha}, \quad Q_{norm} = \frac{Q}{\alpha}
\end{aligned}
\end{equation}

where $I$ and $Q$ are the in-phase and quadrature components, $N$ is the number of samples, and $\alpha$ is the power-based scaling factor. This normalization preserves the signal-to-noise ratio while enabling consistent constellation visualization across different power levels.

The constellation diagram is generated using a 2D histogram approach with logarithmic enhancement:
\begin{equation}
H(i,j) = \log_e(1 + \text{hist2d}(I_{norm}, Q_{norm}, \text{bins}=(64,64)))
\end{equation}

This preprocessing achieves a 5.7× improvement in SNR accuracy compared to per-image normalization, as demonstrated in our ablation studies. The logarithmic transformation enhances the visibility of low-density regions while maintaining the relative power relationships critical for SNR estimation.

\subsection{Architecture Design}

Our architecture employs a ResNet50 backbone with a specialized multi-task head designed for joint modulation-SNR classification. The choice of ResNet50 is motivated by its superior performance on constellation analysis tasks, achieving 48.65\% combined accuracy compared to 38.77\% for Swin Transformer and 34.71\% for ResNet18 in our comprehensive architecture evaluation.

The multi-task head consists of parallel branches for modulation and SNR prediction:

\textbf{Modulation Branch}: A standard classification head with dropout (0.5) and linear projection:
\begin{equation}
\hat{y}_m = \text{Linear}(\text{Dropout}(\text{features})), \quad \hat{y}_m \in \mathbb{R}^{17}
\end{equation}

\textbf{SNR Branch}: A bottleneck architecture with 128-dimensional compression:
\begin{equation}
\begin{aligned}
z &= \text{ReLU}(\text{Linear}(\text{features}, 128)) \\
\hat{y}_s &= \text{Linear}(\text{Dropout}(z, 0.5), 16), \quad \hat{y}_s \in \mathbb{R}^{16}
\end{aligned}
\end{equation}

The bottleneck design forces the SNR branch to learn compact, SNR-specific representations, achieving superior performance compared to standard linear projection (47.15\% vs 46.45\% validation accuracy in our hyperparameter optimization).

\subsection{Training Strategy}

We employ a curriculum learning approach with a sliding window strategy to manage the complexity of joint 272-class classification. The curriculum progressively introduces SNR levels from high to low, allowing the model to first learn clear modulation patterns before tackling noisy scenarios.

The curriculum sampling weight for SNR level $s$ at training step $t$ is:
\begin{equation}
w_s(t) = \begin{cases}
1.0 & \text{if } s \geq s_{\text{max}}(t) \\
\exp\left(-\frac{(s_{\text{max}}(t) - s)^2}{2\sigma_{\text{curr}}^2}\right) & \text{otherwise}
\end{cases}
\end{equation}

where $s_{\text{max}}(t)$ decreases linearly from 30 dB to 0 dB over the first 75\% of training, and $\sigma_{\text{curr}}$ controls the curriculum window width.

Training uses a cyclic learning rate schedule with triangular2 decay:
\begin{equation}
\text{lr}(t) = \text{lr}_{\text{base}} + (\text{lr}_{\text{max}} - \text{lr}_{\text{base}}) \cdot \max(0, 1 - |x|) \cdot \text{scale}
\end{equation}

where $x$ is the cycle position, $\text{lr}_{\text{base}} = 1 \times 10^{-6}$, $\text{lr}_{\text{max}} = 7 \times 10^{-4}$, and scale decreases by half each cycle. This aggressive learning rate range was determined through Bayesian optimization, balancing performance (51.03\% validation accuracy) with training stability.

We implement cycle-aware patience to prevent premature convergence, allowing the model to explore through multiple learning rate cycles before triggering early stopping. The patience mechanism tracks validation improvements over complete cycles rather than individual epochs, preventing early termination during the natural performance fluctuations of cyclic training.

\subsection{Perturbation Impact Score (PIS)}

To provide interpretability for our joint classification decisions, we introduce the Perturbation Impact Score (PIS), a novel explainability metric specifically designed for constellation diagrams. PIS quantifies the importance of constellation regions by measuring the impact of systematic perturbations on model predictions.

For a constellation diagram $X$ with ground truth modulation $m^*$ and SNR $s^*$, we define a perturbation mask $M_{i,j}$ that occludes a local region centered at pixel $(i,j)$ with radius $r$:
\begin{equation}
M_{i,j}(x,y) = \begin{cases}
0 & \text{if } \sqrt{(x-i)^2 + (y-j)^2} \leq r \\
1 & \text{otherwise}
\end{cases}
\end{equation}

The perturbed input is computed as:
\begin{equation}
X_{i,j}^{pert} = X \odot M_{i,j}
\end{equation}

where $\odot$ denotes element-wise multiplication, effectively masking the selected region.

The PIS for position $(i,j)$ combines both modulation and SNR prediction changes:
\begin{equation}
\text{PIS}(i,j) = \alpha \cdot \Delta P_m(i,j) + (1-\alpha) \cdot \Delta P_s(i,j)
\end{equation}

where:
\begin{equation}
\begin{aligned}
\Delta P_m(i,j) &= P_m(m^*|X) - P_m(m^*|X_{i,j}^{pert}) \\
\Delta P_s(i,j) &= P_s(s^*|X) - P_s(s^*|X_{i,j}^{pert})
\end{aligned}
\end{equation}

and $\alpha$ is a weighting parameter that balances modulation and SNR contributions based on task-specific requirements.

The PIS provides intuitive visualizations where higher values indicate regions critical for correct classification. Unlike general-purpose XAI methods, PIS is specifically designed for the dual-task nature of joint modulation-SNR classification, providing separate insights into modulation-specific geometric patterns and SNR-dependent noise characteristics.

[TBD: Add mathematical notation formatting and verify equation numbering]

\section{Results}
\label{sec:results}

Summarize the data collected and the statistical treatment. Use
equations, figures and tables for clarity and brevity. Extensive but relevant data should be included in the supporting information.

\subsection{Equations}


\begin{quote}
	\[{2{x^T}(t)x(t - \tau) = {x^T}(t){Q^{ - \frac{1}{2}}}x(t - \tau) \ le \frac{1}{\alpha }{x^T}(t)Px(t) + \alpha {x^T}(t - \tau)x(t - \tau)}\eqno(1)\]
	\[ i\hbar\frac{\partial \psi}{\partial t}
	= \frac{-\hbar^2}{2m} \left(
	\frac{\partial^2}{\partial x^2}
	+ \frac{\partial^2}{\partial y^2}
	+ \frac{\partial^2}{\partial z^2}
	\right) \psi + V \psi\eqno(2)\]
	%{[}add an equation here; use MS Word or MathType equation function{]}\hfill {(1)}
\end{quote}


\subsection{Tables}

\begin{center}
	\textbf{Table 1.} Table Caption.
\end{center}

\begin{center}
	\begin{longtable}[]{@{}
			>{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.33}}
			>{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.33}}
			>{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.33}}@{}}
		\toprule
		
		\begin{minipage}[b]{\linewidth}\raggedright
			\centering\textbf{Title 1}
		\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
			\centering\textbf{Title 2}
		\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
			\centering\textbf{Title 3}
		\end{minipage} \\
		\midrule
		\endhead
		entry 1 & data \textsuperscript{a} & data \textsuperscript{b} \\
		entry 2 & data & data \\
		\bottomrule
	\end{longtable}
\end{center}
\vspace{-4em}
\textsuperscript{a} Table footnotes should be given as superscript letters such as a, b etc.; \textsuperscript{b} The footnotes should appear at the foot of the table.


\subsection{Figures}

Every figure must have a caption that includes figure numbers and a
brief, informative description, preferably in non-sentence format. Text
in the figure legend should be with same fronts (Times News Roman) and
size (12pt). Figures should be numbered in the order in which they are
referred to in the text, using sequential numerals (e.g. figure 1,
figure 2, etc.). If there is more than one part to a figure (e.g. figure
1(a), figure 1(b), etc.), the parts should be identified by a lower-case
letter in parentheses close to or within the area of the figure. For
figures copied from other publication, permission is needed. Please add
references of the figure caption and put appropriate credit lines like:
Reprinted with permission {[}1{]}. Copyright 2021 Elsevier.


\begin{figure}[H]
	\centering
	\includegraphics[width=4cm,height=3cm]{fig/fig1.png}
	\includegraphics[width=4cm,height=3cm]{fig/fig1.png}
	\caption{ Figure Caption 1 {[}1{]}. Reprinted with permission {[}1{]}. Copyright 2021 Elsevier.}
\end{figure}


\subsection{Enumerate}
\subsubsection{Example}
\textbf{example:}

\begin{enumerate}[(1)]
	\setlength{\itemindent}{2em}
	\item item one
	\item item two
	\item item three
\end{enumerate}

\textbf{another type:}
\begin{enumerate}[(1)]
	\setlength{\itemindent}{2em}
	%\setlength{\labelsep}{2em}
	\item item one
	\begin{itemize}
		\setlength{\itemindent}{2em}
		\item[a.] sub item one
		\item[b.] sub item two
	\end{itemize}
	\item item two
	\item item three
\end{enumerate}

\textbf{bullet type:}
\begin{itemize}
	\setlength{\itemindent}{1em}
	\item[$\bullet$] item one
	\item[$\bullet$] item two
	\item[$\bullet$] item three
\end{itemize}

\section{Conclusion}
%\textbf{\large 4. Conclusion}

The purpose is to interpret and compare results and point out the
features and limitation of the work. Relate the research to current
knowledge in the field.

\section{Supplementary data}

The authors confirm that the supplementary data are available within this article.

\section*{Acknowledgments}
 
Funding: All sources of financial support for the project \textbf{must}
be disclosed in the acknowledgements section. The name of the funding
agency and the grant number should be given, for example: This work was
funded by the National Foundation of Science with grant number
XXX. Author should fill the grant number information during the
submission stage. Please make sure the grant number is correctly
written.

\section*{Author’s contribution}

Please make specific attributions of author
contribution and responsibility in this part and follow
\href{https://casrai.org/credit/}{CRediT} to define the roles of
co-authors.

\section*{Conflicts of Interests}

Authors are required to disclose any potential
conflict(s) of interest like employment, consulting fees, research
contracts, stock ownership, patent licenses, honoraria, advisory
affiliations etc.

\section*{Ethical statement}

For research involving human experiments, please add “The study was performed in accordance with the Declaration of Helsinki and approved by the name of the Ethics Committee or Institutional Review Board (approval date, and approval number must be included)”.
For research involving animal experiments, please add “The study was approved by the name of the Ethics Committee or Institutional Review Board (approval date, and approval number must be included)”.
If ethical approval is not required, authors must provide an exemption from the Ethics Committee or Institutional Review Board, or a detailed statement because approval is not required.


\section*{References}
Direct quotations from another author's work should be cited as
footnote. For publications that are Accepted or submitted or In
preparation, please just add a note at the end of the corresponding
reference.
\setlength{\parindent}{0em}


\textbf{Journal articles}

Journal references must include the author names, abbreviated journal
title, year of publication, volume (optional) and page range. For more
than five authors, the name of the author should be given followed by
\textit{et al}. A collaborative group of authors or by a corporate body is
accepted.

{[}1{]} Rafalskyi, D, Martínez, JM, Habl, L. Title of the paper. \textit{Nature} 2021, 24(2):361‐369. DOI.

\textbf{Books and Book Chapters}

Book references must include the author or the editor names, book title,
publisher, city of the publisher and year of publication.


{[}2{]} Copstead LE, Banasik JL. \textit{Pathophysiology}, 3rd ed. St Louis: Elsevier, 2005. pp.123-126.
{[}3{]} Copstead LE, Banasik JL. Chapter title. In	\textit{Pathophysiology}, 3rd ed. St Louis: Elsevier, 2005. Pp. 123-126.

\textbf{Book Series}

Book series that are periodicals but are not journals could be styled
either as books or journals.

Author 1; Author 2; Author 3; In Title; Editor 1, Editor 2, Eds.; Series
title; Publisher: Place of Publication, Year; Pagination.

{[}4{]} Copstead LE, Banasik JL. \textit{Lignocellulose Biodegradation};
	Saha BC, Hayashi K, Eds. ACS Symposium Series 889; ACS: Washington DC,
	2004.

\textbf{Conference-series}

Conference series should include the title of the conference and the
title of the series but not the publisher.

{[}5{]} Holstein BR Title of the conferences. \textit{J. Phys.: Conf.
		Ser.} 2009, \textbf{173,} 012019

\textbf{Conference proceedings}

Conference proceedings should include the presented work and the
conference proceeding title and the publisher.

{[}6{]} Chiu, AS, Yip, H. From CM to TCM: A case study of the Tung Wah
	Hospital in Hong Kong since 1870 (title of the presentation). In
	\textit{Conference Proceedings}, conference name, Paris, July 6-10,
	2015. City: Publisher, Year of publication. Page rage or article
	number (Optional).

\textbf{Thesis}\\
Thesis should include the title of the thesis. Level of thesis,
university, year.

{[}7{]} Breton, JC Title of the thesis. Ph.D. Degree, University of
	Lille, France, 2018.

\textbf{Patent}

The patent should include the patent owner, title, number and the date.

{[}8{]} Sheem SK. Low-cost Fiber optic pressure sensor. U.S. Patent
	6,738,537, 2004.

\textbf{Dataset}

The citation of the dataset should contain the title of the author.
title of the dataset, the publisher, the place of publication, the data
entry number, URL (if available).

{[}9{]} Department of Premier and Cabinet.\textit{~PROV environmental
		sustainability}.~Melbourne: Public Record Office~Victoria, 2015.
	Available:~http://data.vic.gov.au/data/dataset/prov-environmental-sustainability
	(accessed on Day Month Year).

\textbf{Website}

General websites are not recommended. If you have to cite the website,
please use this format.

{[}10{]} Author (If any). Title of the website. URL (accessed on Day
	Month Year).

 
  

\end{document}
	